#!/usr/bin/env python3
"""
Test script for the new reporting functionality.
"""

import asyncio
import sys
import tempfile
from pathlib import Path

# Add src to Python path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from reporting.project_reporter import ProjectReporter
from assembly.project_generator import ProjectGenerator


async def test_reporting_system():
    """Test the comprehensive reporting system."""
    
    print("ğŸ§ª Testing AutoBot Assembly Reporting System")
    print("=" * 60)
    
    try:
        # Create a test project
        with tempfile.TemporaryDirectory() as temp_dir:
            generator = ProjectGenerator()
            
            # Sample project files
            test_files = {
                'main.py': '''#!/usr/bin/env python3
"""
News Scraper Application

A web scraper that extracts news headlines from multiple websites.
Generated by AutoBot Assembly System.
"""

import requests
from bs4 import BeautifulSoup
import json
from typing import List, Dict

class NewsScraper:
    """Web scraper for news headlines."""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (compatible; NewsBot/1.0)'
        })
    
    def scrape_headlines(self, url: str) -> List[Dict[str, str]]:
        """Scrape headlines from a news website."""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            headlines = []
            
            # Common headline selectors
            selectors = ['h1', 'h2', '.headline', '.title']
            
            for selector in selectors:
                elements = soup.select(selector)
                for element in elements[:10]:
                    text = element.get_text(strip=True)
                    if len(text) > 10:
                        headlines.append({
                            'title': text,
                            'url': url,
                            'selector': selector
                        })
                
                if headlines:
                    break
            
            return headlines
            
        except Exception as e:
            print(f"Error scraping {url}: {e}")
            return []

def main():
    """Main function."""
    scraper = NewsScraper()
    
    test_urls = [
        'https://example-news-site.com',
        'https://another-news-site.com'
    ]
    
    all_headlines = []
    
    for url in test_urls:
        print(f"Scraping {url}...")
        headlines = scraper.scrape_headlines(url)
        all_headlines.extend(headlines)
        print(f"Found {len(headlines)} headlines")
    
    # Save results
    with open('headlines.json', 'w') as f:
        json.dump(all_headlines, f, indent=2)
    
    print(f"Total headlines collected: {len(all_headlines)}")
    print("Results saved to headlines.json")

if __name__ == "__main__":
    main()
''',
                'requirements.txt': '''requests>=2.25.0
beautifulsoup4>=4.9.0
lxml>=4.6.0
''',
                'README.md': '''# News Scraper

A Python web scraper that extracts news headlines from multiple websites.

## Installation

```bash
pip install -r requirements.txt
```

## Usage

```bash
python main.py
```

## Features

- Multi-website scraping
- Intelligent headline detection
- JSON output format
- Error handling and logging

## Generated by AutoBot Assembly System

- **Project Type**: cli_tool
- **Language**: python
- **Complexity**: 0.7/1.0
- **Estimated Effort**: 2-4 hours
''',
                'config.json': '''{"app_name": "news_scraper", "version": "1.0.0", "max_headlines": 50}''',
                'tests/test_scraper.py': '''import unittest
from main import NewsScraper

class TestNewsScraper(unittest.TestCase):
    def setUp(self):
        self.scraper = NewsScraper()
    
    def test_scraper_initialization(self):
        self.assertIsNotNone(self.scraper.session)
    
    def test_empty_url_handling(self):
        headlines = self.scraper.scrape_headlines("")
        self.assertEqual(headlines, [])

if __name__ == '__main__':
    unittest.main()
''',
                '.gitignore': '''__pycache__/
*.pyc
*.pyo
*.pyd
.Python
env/
venv/
.venv/
.env
headlines.json
'''
            }
            
            # Sample repository information
            repositories = [
                {
                    'name': 'beautiful-soup-scraper',
                    'url': 'https://github.com/example/beautiful-soup-scraper',
                    'files_copied': ['scraper.py', 'utils.py'],
                    'purpose': 'Web scraping framework with BeautifulSoup',
                    'license': 'MIT'
                },
                {
                    'name': 'news-aggregator',
                    'url': 'https://github.com/example/news-aggregator',
                    'files_copied': ['headline_parser.py', 'config.json'],
                    'purpose': 'News headline parsing and aggregation',
                    'license': 'Apache-2.0'
                }
            ]
            
            # Generate project with comprehensive report
            print("ğŸ“ Generating test project with comprehensive report...")
            project = generator.generate_project(
                project_name="NewsScraperDemo",
                output_dir=temp_dir,
                files=test_files,
                repositories=repositories,
                generate_report=True
            )
            
            print(f"âœ… Project generated successfully!")
            print(f"   Name: {project.name}")
            print(f"   Path: {project.path}")
            print(f"   Files: {len(project.files)}")
            print(f"   Size: {project.size} bytes")
            
            if project.report_path:
                print(f"   Report: {project.report_path}")
                
                # Read and display part of the report
                with open(project.report_path, 'r') as f:
                    report_content = f.read()
                
                print("\nğŸ“Š Generated Report Preview:")
                print("-" * 40)
                # Show first 1000 characters of the report
                print(report_content[:1000])
                if len(report_content) > 1000:
                    print("...")
                    print(f"\n[Report continues for {len(report_content) - 1000} more characters]")
                
                print(f"\nâœ… Full report saved to: {project.report_path}")
                
                # Check if JSON report was also created
                json_report_path = Path(project.report_path).with_suffix('.json')
                if json_report_path.exists():
                    print(f"âœ… JSON report saved to: {json_report_path}")
            
            return True
            
    except Exception as e:
        print(f"âŒ Reporting test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run the reporting system test."""
    
    print("ğŸ§ª AutoBot Assembly System - Reporting Test")
    print("=" * 60)
    
    success = await test_reporting_system()
    
    if success:
        print("\nğŸ‰ REPORTING SYSTEM TEST PASSED!")
        print("\nğŸ“Š New Features Added:")
        print("âœ… Comprehensive project analysis")
        print("âœ… File structure documentation")
        print("âœ… Repository source tracking")
        print("âœ… Development plan generation")
        print("âœ… Next steps recommendations")
        print("âœ… Technology stack identification")
        print("âœ… Markdown and JSON report formats")
        print("\nğŸš€ The AutoBot Assembly System now generates detailed reports!")
    else:
        print("\nâŒ Reporting system test failed.")


if __name__ == "__main__":
    asyncio.run(main())