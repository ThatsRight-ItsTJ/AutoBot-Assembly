#!/usr/bin/env python3
"""
AutoBot CLI Interface

Command-line interface for the AutoBot Assembly System.
"""

import argparse
import asyncio
import sys
import logging
from pathlib import Path
from typing import Optional, Dict, Any
import json
import os
from datetime import datetime

# Add the project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Now import with absolute imports
from src.orchestration.search_orchestrator import SearchOrchestrator
from src.assembly.project_generator import ProjectGenerator
from src.reporting.ai_integrated_reporter import AIIntegratedReporter


class AutoBotCLI:
    """Command-line interface for AutoBot Assembly System."""
    
    def __init__(self):
        self.orchestrator = SearchOrchestrator()
        self.generator = ProjectGenerator()
        self.reporter = AIIntegratedReporter()
        
    def create_parser(self) -> argparse.ArgumentParser:
        """Create the argument parser."""
        parser = argparse.ArgumentParser(
            description="AutoBot Assembly System - Automated project generation",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Examples:
  python -m src.cli.autobot_cli batch "Create a web scraper"
  python -m src.cli.autobot_cli interactive
  python -m src.cli.autobot_cli wizard --type web_service
            """
        )
        
        # Mode selection - make this a positional argument
        parser.add_argument(
            'mode',
            choices=['interactive', 'wizard', 'batch'],
            help='Operation mode'
        )
        
        # Prompt for batch mode
        parser.add_argument(
            'prompt',
            nargs='?',
            help='Project description (required for batch mode)'
        )
        
        # Optional arguments
        parser.add_argument(
            '--output', '-o',
            type=str,
            default='./generated_project',
            help='Output directory for generated project'
        )
        
        parser.add_argument(
            '--type', '-t',
            choices=['application', 'library', 'web_service', 'cli_tool'],
            default='application',
            help='Project type'
        )
        
        parser.add_argument(
            '--language', '-l',
            choices=['python', 'javascript', 'java'],
            default='python',
            help='Programming language'
        )
        
        parser.add_argument(
            '--verbose', '-v',
            action='store_true',
            help='Enable verbose output'
        )
        
        parser.add_argument(
            '--skip-tests',
            action='store_true',
            help='Skip test generation'
        )
        
        parser.add_argument(
            '--skip-docs',
            action='store_true',
            help='Skip documentation generation'
        )
        
        parser.add_argument(
            '--max-repos',
            type=int,
            default=10,
            help='Maximum repositories to analyze'
        )
        
        parser.add_argument(
            '--timeout',
            type=int,
            default=300,
            help='Timeout in seconds'
        )
        
        return parser
    
    def _generate_project_files(self, search_results, project_config) -> Dict[str, str]:
        """Generate project files based on search results and config."""
        files = {}
        
        # Generate main scraper file
        if project_config['language'] == 'python':
            files['main.py'] = self._generate_python_scraper(search_results, project_config)
            files['requirements.txt'] = self._generate_requirements(search_results)
            files['README.md'] = self._generate_readme(project_config)
            
            if project_config.get('include_tests', True):
                files['test_main.py'] = self._generate_test_file(project_config)
        
        return files
    
    def _generate_python_scraper(self, search_results, project_config) -> str:
        """Generate a Python web scraper based on the requirements."""
        return '''#!/usr/bin/env python3
"""
Web Scraper for News Headlines

This script scrapes news headlines from multiple websites and saves them to JSON.
Generated by AutoBot Assembly System.
"""

import requests
from bs4 import BeautifulSoup
import json
import logging
from datetime import datetime
from typing import List, Dict
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NewsHeadlineScraper:
    """Scraper for extracting news headlines from multiple websites."""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Sample news websites (you can extend this list)
        self.news_sources = [
            {
                'name': 'Example News 1',
                'url': 'https://example-news-1.com',
                'headline_selector': 'h2.headline, .article-title, h1'
            },
            {
                'name': 'Example News 2', 
                'url': 'https://example-news-2.com',
                'headline_selector': '.news-headline, h2.title, .story-title'
            }
        ]
    
    def scrape_headlines(self, source: Dict) -> List[str]:
        """Scrape headlines from a single news source."""
        headlines = []
        
        try:
            logger.info(f"Scraping headlines from {source['name']}")
            response = self.session.get(source['url'], timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            headline_elements = soup.select(source['headline_selector'])
            
            for element in headline_elements[:10]:  # Limit to top 10 headlines
                headline = element.get_text(strip=True)
                if headline and len(headline) > 10:  # Filter out short/empty headlines
                    headlines.append(headline)
            
            logger.info(f"Found {len(headlines)} headlines from {source['name']}")
            
        except Exception as e:
            logger.error(f"Error scraping {source['name']}: {str(e)}")
        
        return headlines
    
    def scrape_all_sources(self) -> Dict:
        """Scrape headlines from all configured news sources."""
        all_headlines = {
            'timestamp': datetime.now().isoformat(),
            'sources': []
        }
        
        for source in self.news_sources:
            headlines = self.scrape_headlines(source)
            
            source_data = {
                'name': source['name'],
                'url': source['url'],
                'headlines_count': len(headlines),
                'headlines': headlines
            }
            
            all_headlines['sources'].append(source_data)
            
            # Be respectful - add delay between requests
            time.sleep(1)
        
        return all_headlines
    
    def save_to_json(self, data: Dict, filename: str = 'news_headlines.json'):
        """Save scraped headlines to JSON file."""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Headlines saved to {filename}")
            
        except Exception as e:
            logger.error(f"Error saving to JSON: {str(e)}")

def main():
    """Main function to run the news scraper."""
    scraper = NewsHeadlineScraper()
    
    logger.info("Starting news headline scraping...")
    headlines_data = scraper.scrape_all_sources()
    
    # Save to JSON
    scraper.save_to_json(headlines_data)
    
    # Print summary
    total_headlines = sum(source['headlines_count'] for source in headlines_data['sources'])
    logger.info(f"Scraping complete! Total headlines collected: {total_headlines}")
    
    return headlines_data

if __name__ == '__main__':
    main()
'''
    
    def _generate_requirements(self, search_results) -> str:
        """Generate requirements.txt based on search results."""
        requirements = [
            'requests>=2.28.0',
            'beautifulsoup4>=4.11.0',
            'lxml>=4.9.0'
        ]
        
        # Add selenium if found in search results
        for result in search_results.all_results:
            if hasattr(result, 'name') and 'selenium' in result.name.lower():
                requirements.append('selenium>=4.0.0')
                break
        
        return '\n'.join(requirements) + '\n'
    
    def _generate_readme(self, project_config) -> str:
        """Generate README.md file."""
        project_name = project_config.get('name', 'Web Scraper')
        return f'''# {project_name.replace('_', ' ').title()}

{project_config.get('description', 'A web scraper for extracting news headlines')}

## Generated by AutoBot Assembly System

This project was automatically generated based on your requirements.

## Installation

```bash
pip install -r requirements.txt
```

## Usage

```bash
python main.py
```

This will scrape news headlines from configured sources and save them to `news_headlines.json`.

## Features

- Scrapes headlines from multiple news websites
- Saves results to JSON format
- Includes error handling and logging
- Respectful scraping with delays between requests

## Configuration

You can modify the `news_sources` list in `main.py` to add your own news websites:

```python
self.news_sources = [
    {{
        'name': 'Your News Site',
        'url': 'https://your-news-site.com',
        'headline_selector': '.headline-class'
    }}
]
```

## Output Format

The scraper saves headlines in the following JSON format:

```json
{{
    "timestamp": "2023-XX-XXTXX:XX:XX",
    "sources": [
        {{
            "name": "News Source Name",
            "url": "https://source-url.com",
            "headlines_count": 10,
            "headlines": ["Headline 1", "Headline 2", ...]
        }}
    ]
}}
```

## Notes

- This is a demo scraper with example URLs
- Replace the example URLs with real news websites
- Respect robots.txt and terms of service of target websites
- Consider adding rate limiting for production use
'''
    
    def _generate_test_file(self, project_config) -> str:
        """Generate test file."""
        return '''#!/usr/bin/env python3
"""
Tests for the news headline scraper.
"""

import unittest
from unittest.mock import patch, MagicMock
import json
from main import NewsHeadlineScraper

class TestNewsHeadlineScraper(unittest.TestCase):
    """Test cases for NewsHeadlineScraper."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.scraper = NewsHeadlineScraper()
    
    def test_scraper_initialization(self):
        """Test scraper initializes correctly."""
        self.assertIsNotNone(self.scraper.session)
        self.assertTrue(len(self.scraper.news_sources) > 0)
    
    @patch('requests.Session.get')
    def test_scrape_headlines_success(self, mock_get):
        """Test successful headline scraping."""
        # Mock response
        mock_response = MagicMock()
        mock_response.content = '<html><h2 class="headline">Test Headline</h2></html>'
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response
        
        source = {
            'name': 'Test Source',
            'url': 'https://test.com',
            'headline_selector': 'h2.headline'
        }
        
        headlines = self.scraper.scrape_headlines(source)
        self.assertEqual(len(headlines), 1)
        self.assertEqual(headlines[0], 'Test Headline')
    
    def test_save_to_json(self):
        """Test JSON saving functionality."""
        test_data = {
            'timestamp': '2023-01-01T00:00:00',
            'sources': []
        }
        
        filename = 'test_output.json'
        self.scraper.save_to_json(test_data, filename)
        
        # Verify file was created and contains correct data
        with open(filename, 'r') as f:
            saved_data = json.load(f)
        
        self.assertEqual(saved_data['timestamp'], test_data['timestamp'])
        
        # Clean up
        import os
        os.remove(filename)

if __name__ == '__main__':
    unittest.main()
'''

    async def run_batch_mode(self, args: argparse.Namespace) -> None:
        """Run in batch mode with the provided prompt."""
        if not args.prompt:
            print("Error: Prompt is required for batch mode")
            print("Usage: python -m src.cli.autobot_cli batch \"Your project description\"")
            sys.exit(1)
        
        print(f"ðŸš€ AutoBot Assembly System - Batch Mode")
        print(f"ðŸ“ Project: {args.prompt}")
        print(f"ðŸ”§ Language: {args.language}")
        print(f"ðŸ“ Output: {args.output}")
        print("-" * 50)
        
        try:
            # Step 1: Search for components
            print("ðŸ” Searching for components...")
            
            # Extract components from the prompt (simple keyword extraction)
            components = self._extract_components(args.prompt)
            
            search_results = await self.orchestrator.orchestrate_search(
                project_name=args.prompt,
                language=args.language,
                components=components,
                max_results_per_tier=args.max_repos
            )
            
            print(f"âœ… Found {len(search_results.all_results)} components")
            
            # Step 2: Generate project
            print("ðŸ—ï¸ Generating project...")
            project_config = {
                'name': self._extract_project_name(args.prompt),
                'description': args.prompt,
                'language': args.language,
                'type': args.type,
                'include_tests': not args.skip_tests,
                'include_docs': not args.skip_docs,
            }
            
            # Generate project files based on search results
            project_files = self._generate_project_files(search_results, project_config)
            
            # Use the existing generate_project method
            generated_project = await self.generator.generate_project(
                project_name=project_config['name'],
                output_dir=args.output,
                files=project_files,
                project_description=project_config['description'],
                language=project_config['language']
            )
            
            print(f"âœ… Project generated: {generated_project.path}")
            
            # Step 3: Generate report (simplified since we don't have the full reporter)
            print("ðŸ“Š Generating analysis report...")
            
            # Create a simple report
            report_data = {
                'project_name': project_config['name'],
                'description': project_config['description'],
                'language': project_config['language'],
                'components_found': len(search_results.all_results),
                'files_generated': list(project_files.keys()),
                'search_results': {
                    'packages': len(search_results.packages),
                    'curated_collections': len(search_results.curated_collections),
                    'discovered_repositories': len(search_results.discovered_repositories)
                },
                'timestamp': datetime.now().isoformat()
            }
            
            # Save report
            report_path = Path(generated_project.path) / "analysis_report.json"
            with open(report_path, 'w') as f:
                json.dump(report_data, f, indent=2, default=str)
            
            print(f"âœ… Report saved: {report_path}")
            
            # Summary
            print("\nðŸŽ‰ Project Generation Complete!")
            print(f"ðŸ“ Project Location: {generated_project.path}")
            print(f"ðŸ“Š Components Used: {len(search_results.all_results)}")
            print(f"ðŸ“„ Files Generated: {len(project_files)}")
            print(f"ðŸ“‹ Report: {report_path}")
            
            # Show key files
            print("\nðŸ“„ Key Files Generated:")
            for file_name in sorted(project_files.keys()):
                print(f"  â€¢ {file_name}")
            
        except Exception as e:
            print(f"âŒ Error: {str(e)}")
            if args.verbose:
                import traceback
                traceback.print_exc()
            sys.exit(1)
    
    def _extract_components(self, prompt: str) -> list:
        """Extract components/features from the project prompt."""
        # Simple keyword-based extraction
        keywords = {
            'web scraper': ['scraping', 'requests', 'beautifulsoup', 'selenium'],
            'web scraping': ['scraping', 'requests', 'beautifulsoup', 'selenium'],
            'scraper': ['scraping', 'requests', 'beautifulsoup'],
            'news': ['news', 'rss', 'feeds'],
            'headlines': ['headlines', 'news'],
            'json': ['json', 'data'],
            'api': ['api', 'rest', 'fastapi', 'flask'],
            'database': ['database', 'sql', 'sqlite', 'postgresql'],
            'cli': ['cli', 'command-line', 'argparse'],
            'gui': ['gui', 'tkinter', 'qt', 'kivy'],
            'machine learning': ['ml', 'sklearn', 'tensorflow', 'pytorch'],
            'data analysis': ['pandas', 'numpy', 'matplotlib'],
        }
        
        prompt_lower = prompt.lower()
        components = []
        
        for key, values in keywords.items():
            if key in prompt_lower:
                components.extend(values)
        
        # If no specific components found, use generic ones based on prompt words
        if not components:
            words = prompt_lower.split()
            components = [word for word in words if len(word) > 3 and word.isalpha()][:5]
        
        return list(set(components))  # Remove duplicates
    
    async def run_interactive_mode(self, args: argparse.Namespace) -> None:
        """Run in interactive mode."""
        print("ðŸš€ AutoBot Assembly System - Interactive Mode")
        print("Type 'help' for commands, 'quit' to exit")
        
        while True:
            try:
                user_input = input("\nAutoBot> ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'q']:
                    print("Goodbye!")
                    break
                elif user_input.lower() == 'help':
                    self._show_help()
                elif user_input.startswith('generate '):
                    prompt = user_input[9:].strip()
                    if prompt:
                        # Create a temporary args object for batch processing
                        batch_args = argparse.Namespace(
                            prompt=prompt,
                            output=args.output,
                            language=args.language,
                            type=args.type,
                            skip_tests=args.skip_tests,
                            skip_docs=args.skip_docs,
                            max_repos=args.max_repos,
                            timeout=args.timeout,
                            verbose=args.verbose
                        )
                        await self.run_batch_mode(batch_args)
                    else:
                        print("Please provide a project description")
                else:
                    print("Unknown command. Type 'help' for available commands.")
                    
            except KeyboardInterrupt:
                print("\nGoodbye!")
                break
            except Exception as e:
                print(f"Error: {str(e)}")
    
    async def run_wizard_mode(self, args: argparse.Namespace) -> None:
        """Run in wizard mode with guided prompts."""
        print("ðŸ§™ AutoBot Assembly System - Wizard Mode")
        print("Let's create your project step by step...\n")
        
        # Collect project details
        project_type = input(f"Project type [{args.type}]: ").strip() or args.type
        language = input(f"Programming language [{args.language}]: ").strip() or args.language
        
        print("\nDescribe your project:")
        description = input("Project description: ").strip()
        
        if not description:
            print("Project description is required!")
            return
        
        output_dir = input(f"Output directory [{args.output}]: ").strip() or args.output
        
        # Confirm settings
        print(f"\nðŸ“‹ Project Configuration:")
        print(f"  Type: {project_type}")
        print(f"  Language: {language}")
        print(f"  Description: {description}")
        print(f"  Output: {output_dir}")
        
        confirm = input("\nProceed with generation? [Y/n]: ").strip().lower()
        if confirm and confirm != 'y' and confirm != 'yes':
            print("Cancelled.")
            return
        
        # Create args for batch processing
        batch_args = argparse.Namespace(
            prompt=description,
            output=output_dir,
            language=language,
            type=project_type,
            skip_tests=args.skip_tests,
            skip_docs=args.skip_docs,
            max_repos=args.max_repos,
            timeout=args.timeout,
            verbose=args.verbose
        )
        
        await self.run_batch_mode(batch_args)
    
    def _show_help(self) -> None:
        """Show interactive mode help."""
        print("""
Available commands:
  generate <description>  - Generate a project from description
  help                   - Show this help message
  quit/exit/q           - Exit the program

Examples:
  generate Create a web scraper for news sites
  generate Build a REST API with FastAPI
  generate Make a CLI tool for file processing
        """)
    
    def _extract_project_name(self, prompt: str) -> str:
        """Extract a project name from the prompt."""
        # Simple extraction - take first few words and clean them
        words = prompt.lower().split()[:3]
        name = '_'.join(word.strip('.,!?') for word in words if word.isalnum())
        return name or 'autobot_project'
    
    async def run(self) -> None:
        """Main entry point."""
        parser = self.create_parser()
        args = parser.parse_args()
        
        # Set up logging
        log_level = logging.DEBUG if args.verbose else logging.INFO
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # Run the appropriate mode
        try:
            if args.mode == 'batch':
                await self.run_batch_mode(args)
            elif args.mode == 'interactive':
                await self.run_interactive_mode(args)
            elif args.mode == 'wizard':
                await self.run_wizard_mode(args)
        except KeyboardInterrupt:
            print("\nOperation cancelled.")
            sys.exit(1)
        except Exception as e:
            print(f"Fatal error: {str(e)}")
            if args.verbose:
                import traceback
                traceback.print_exc()
            sys.exit(1)


def main():
    """Entry point for the CLI."""
    cli = AutoBotCLI()
    asyncio.run(cli.run())


if __name__ == '__main__':
    main()