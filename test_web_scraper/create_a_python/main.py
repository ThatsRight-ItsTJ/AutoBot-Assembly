#!/usr/bin/env python3
"""
Web Scraper for News Headlines

This script scrapes news headlines from multiple websites and saves them to JSON.
Generated by AutoBot Assembly System.
"""

import requests
from bs4 import BeautifulSoup
import json
import logging
from datetime import datetime
from typing import List, Dict
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NewsHeadlineScraper:
    """Scraper for extracting news headlines from multiple websites."""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Sample news websites (you can extend this list)
        self.news_sources = [
            {
                'name': 'Example News 1',
                'url': 'https://example-news-1.com',
                'headline_selector': 'h2.headline, .article-title, h1'
            },
            {
                'name': 'Example News 2', 
                'url': 'https://example-news-2.com',
                'headline_selector': '.news-headline, h2.title, .story-title'
            }
        ]
    
    def scrape_headlines(self, source: Dict) -> List[str]:
        """Scrape headlines from a single news source."""
        headlines = []
        
        try:
            logger.info(f"Scraping headlines from {source['name']}")
            response = self.session.get(source['url'], timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            headline_elements = soup.select(source['headline_selector'])
            
            for element in headline_elements[:10]:  # Limit to top 10 headlines
                headline = element.get_text(strip=True)
                if headline and len(headline) > 10:  # Filter out short/empty headlines
                    headlines.append(headline)
            
            logger.info(f"Found {len(headlines)} headlines from {source['name']}")
            
        except Exception as e:
            logger.error(f"Error scraping {source['name']}: {str(e)}")
        
        return headlines
    
    def scrape_all_sources(self) -> Dict:
        """Scrape headlines from all configured news sources."""
        all_headlines = {
            'timestamp': datetime.now().isoformat(),
            'sources': []
        }
        
        for source in self.news_sources:
            headlines = self.scrape_headlines(source)
            
            source_data = {
                'name': source['name'],
                'url': source['url'],
                'headlines_count': len(headlines),
                'headlines': headlines
            }
            
            all_headlines['sources'].append(source_data)
            
            # Be respectful - add delay between requests
            time.sleep(1)
        
        return all_headlines
    
    def save_to_json(self, data: Dict, filename: str = 'news_headlines.json'):
        """Save scraped headlines to JSON file."""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Headlines saved to {filename}")
            
        except Exception as e:
            logger.error(f"Error saving to JSON: {str(e)}")

def main():
    """Main function to run the news scraper."""
    scraper = NewsHeadlineScraper()
    
    logger.info("Starting news headline scraping...")
    headlines_data = scraper.scrape_all_sources()
    
    # Save to JSON
    scraper.save_to_json(headlines_data)
    
    # Print summary
    total_headlines = sum(source['headlines_count'] for source in headlines_data['sources'])
    logger.info(f"Scraping complete! Total headlines collected: {total_headlines}")
    
    return headlines_data

if __name__ == '__main__':
    main()
